{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "34eb36ca-432c-407e-a0dc-0b36e8d00355",
      "metadata": {
        "id": "34eb36ca-432c-407e-a0dc-0b36e8d00355"
      },
      "source": [
        "# Exploring Different Methods in Extracting Contents That Were Stuck in PDFs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "240cd8d8-8f1c-435d-a41a-f7faa3a1170c",
      "metadata": {
        "id": "240cd8d8-8f1c-435d-a41a-f7faa3a1170c"
      },
      "source": [
        "### 1. Using package `pdftools` in R to extract text\n",
        "**Main Steps:** \\\n",
        "• Use `pdf_text` to extract all text from a pdf\\\n",
        "• Remove multiple spaces\\\n",
        "• Use RegEx to split and select the information that we need\\\n",
        "• Output the list of text, and each element should be the content of a table that we want\\\n",
        "• Further clean the text(hard to achieve due to the poor quality of pdf)\\\n",
        "• Convert text to dataframe(haven’t got this far)\n",
        "\n",
        "**Pros:** Input is pdf file. No need to convert it to immages.\\\n",
        "**Cons:** The performance of RegEx method and `pdf_text`function is highly depends on the quality of input. We cannot improve their ability of reading the text since they are not training models.\n",
        "\n",
        "Click [here](https://drive.google.com/file/d/1bg73-6Najxc-K1_PM63pzAUkyNXK5_gp/view?usp=drive_link) to read the code in R."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19d819fa-63fe-4b56-bffc-86068b862d4e",
      "metadata": {
        "id": "19d819fa-63fe-4b56-bffc-86068b862d4e"
      },
      "source": [
        "### 2. Using Tabula\n",
        "Tabula is a free and open-source tool designed for extracting tables from PDF files and converting them into more computation readable formats like CSV or JSON. This software is particularly useful for journalists, researchers, and data analysts who need to work with data trapped inside PDF documents, a format that is not inherently conducive to data analysis. You can find more information [here](https://tabula.technology/).\n",
        "\n",
        "**Main Steps:**\n",
        "- Download and install Tabula app\n",
        "- Upload the pdf to Tabula app\n",
        "- Manually select the areas from which we want to extract the contents\n",
        "- Export the output in a format that you need (i.e. csv, zip, json)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8b0e45c-1cfe-4c7e-ab61-4a39626de36a",
      "metadata": {
        "id": "d8b0e45c-1cfe-4c7e-ab61-4a39626de36a"
      },
      "source": [
        "**Example 1: P00301_003#1.pdf**(better outcome)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a10e708-89c0-47de-8698-3c047699ec4f",
      "metadata": {
        "id": "0a10e708-89c0-47de-8698-3c047699ec4f"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e6fd5ed-dff0-491b-9cb5-fd67cafc2462",
      "metadata": {
        "id": "9e6fd5ed-dff0-491b-9cb5-fd67cafc2462"
      },
      "outputs": [],
      "source": [
        "# Replace 'your_zipfile.zip' with the path to your ZIP file\n",
        "zip_file_path = 'P00301_003.zip'\n",
        "\n",
        "# Open the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    # List to hold dataframes\n",
        "    dfs = []\n",
        "\n",
        "    # Iterate through each file in the ZIP\n",
        "    for filename in z.namelist():\n",
        "        # Check if the file is a CSV\n",
        "        if filename.endswith('.csv'):\n",
        "            # Read the CSV file\n",
        "            with z.open(filename) as f:\n",
        "                df = pd.read_csv(f, header = None)\n",
        "                dfs.append(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# manually adding headers\n",
        "col_names = [\"lable_point\", \"delta_time\", \"presure_(PSI)\", \"T+DT/DT\", \"log\", \"Pw_Pf(PSI)\", \"comments\"]\n",
        "dfs[0].columns = col_names\n",
        "\n",
        "# remove spaces within each observations\n",
        "dfs[0] = dfs[0].applymap(lambda x: x.replace(' ', '') if isinstance(x, str) else x)\n",
        "\n",
        "#manually fix some misreadings\n",
        "dfs[0][\"delta_time\"][16] = 55\n",
        "dfs[0][\"presure_(PSI)\"][19] = 1740.4"
      ],
      "metadata": {
        "id": "1vluSnDO9RkB"
      },
      "id": "1vluSnDO9RkB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2756cb11-8eaf-402e-9247-284fde314ed6",
      "metadata": {
        "id": "2756cb11-8eaf-402e-9247-284fde314ed6"
      },
      "source": [
        " **Example 2: P00315_A004**(worse outcome)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "715248a4-458d-4be7-ae14-058dee855cd5",
      "metadata": {
        "id": "715248a4-458d-4be7-ae14-058dee855cd5"
      },
      "outputs": [],
      "source": [
        "# Replace 'your_zipfile.zip' with the path to your ZIP file\n",
        "zip_file_path = 'P00315_A004.zip'\n",
        "\n",
        "# Open the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    # List to hold dataframes\n",
        "    dfs2 = []\n",
        "\n",
        "    # Iterate through each file in the ZIP\n",
        "    for filename in z.namelist():\n",
        "        # Check if the file is a CSV\n",
        "        if filename.endswith('.csv'):\n",
        "            # Read the CSV file\n",
        "            with z.open(filename) as f:\n",
        "                df2 = pd.read_csv(f, header = None)\n",
        "                dfs2.append(df2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd99fed0-5648-4d5f-9827-35c6abfeefba",
      "metadata": {
        "id": "cd99fed0-5648-4d5f-9827-35c6abfeefba"
      },
      "source": [
        "The output for this file was relatively less accurate than the one from example 1. It merged the first two columns and the last two, and it failed to capture most decimals. This happened due to the poor quality of the pdfs and the inconsistent distance between columns.\n",
        "\n",
        "**Pros and Cons**\n",
        "\n",
        "Pros: It is free. Processes are easy and straightforward. Output could be exported in different formats.\n",
        "\n",
        "Cons: The outcome highly depends on the quality."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "anaconda-panel-2023.05-py310",
      "language": "python",
      "name": "conda-env-anaconda-panel-2023.05-py310-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}